<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=HandheldFriendly content="True"><meta http-equiv=x-ua-compatible content="IE=edge"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=generator content="Hugo 0.97.3"><link rel="shortcut icon" href=https://cdn.jsdelivr.net/gh/dsrkafuu/dsr-cdn-main@1/images/favicons/dsrca.ico><title>Rediscovering Sorting's Holy Grail - ecstaticmorse</title><meta name=author content="Dylan MacKenzie"><meta property="og:title" content="Rediscovering Sorting's Holy Grail"><meta name=twitter:title content="Rediscovering Sorting's Holy Grail"><meta property="og:type" content="article"><meta property="og:url" content="//ecstaticmorse.net/posts/grailsort/"><meta property="og:description" content="Implementing a stable, linearithmic sort in constant space."><meta name=twitter:description content="Implementing a stable, linearithmic sort in constant space."><meta name=twitter:card content="summary"><meta property="article:published_time" content="2022-03-23T14:03:03-07:00"><meta property="article:modified_time" content="2022-03-23T14:03:03-07:00"><style>@media(prefers-color-scheme:dark){body[data-theme=auto] img{filter:brightness(80%)}}body[data-theme=dark] img{filter:brightness(80%)}</style><link rel=stylesheet href=//ecstaticmorse.net/assets/css/fuji.min.css></head><body data-theme=auto data-theme-auto=true><script data-cfasync=false>var fujiThemeData=localStorage.getItem("fuji_data-theme");fujiThemeData?fujiThemeData!=="auto"&&document.body.setAttribute("data-theme",fujiThemeData==="dark"?"dark":"light"):localStorage.setItem("fuji_data-theme","auto")</script><header><div class="container-lg clearfix"><div class="col-12 header"><a class=title-main href=//ecstaticmorse.net/>ecstaticmorse</a></div></div></header><main><div class="container-lg clearfix"><div class="col-12 col-md-9 float-left content"><article><h2 class="post-item post-title"><a href=//ecstaticmorse.net/posts/grailsort/>Rediscovering Sorting's Holy Grail</a></h2><div class="post-item post-meta"><span><i class="iconfont icon-today-sharp"></i>&nbsp;2022-03-23</span>
<span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;No tag</span></div><div class="post-content markdown-body"><h6 id=dedication><a class=heading-link href=#dedication>Dedication</a></h6><p>What follows is dedicated to Andrey Astrelin. After a life dedicated to the
pursuit of knowledge—recounted by his family <a href=http://superliminal.com/andrey/biography.html target=_blank>here</a>—Andrey passed away in
2017.</p><h2 id=introduction><a class=heading-link href=#introduction>Introduction</a></h2><p>When I learned about sorting algorithms, I was given the impression
that one could have at most two out of the following three:</p><ul><li>stability</li><li>$O(n \log{} n)$ runtime</li><li>$O(1)$ space</li></ul><p>Sorting is one of the first topics covered in any introductory computer science
course, so you can imagine my surprise when I learned that this was false!
Worse, I was not alone in this misconception; here is a slide from Robert
Sedgewick&rsquo;s algorithms course at Princeton (circa 2010):</p><p><img class=img-zoomable src=/images/grailsort/grail-sort-slide.png alt="A slide from Sedgewick&amp;rsquo;s Algorithms course at Princeton, depicting sorting algorithms with various properties as well as yet-to-be-discovered &amp;ldquo;holy sorting grail&amp;rdquo;"></p><p>I first saw this slide in a <a href=https://github.com/Mrrl/GrailSort target=_blank>blog post</a> by Andrey Astrelin (AKA
Mrrl). It goes on to describe the implementation of a sorting algorithm
fulfilling all three criteria. Andrey called his implementation <a href=https://github.com/Mrrl/GrailSort target=_blank>grailsort</a>, a
reference to the final row in the table above. As it turns out, a &ldquo;holy sorting
grail&rdquo; was described as early as 1974 by Luis Trabb Pardo.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>
Andrey implemented a more recent version from Bing-Chao Huang and Michael A.
Langston.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> Both papers use similar techniques, so
I&rsquo;ll refer to this family of algorithms as <a href=https://en.wikipedia.org/wiki/Block_sort target=_blank>block sort</a>.</p><p>In this post, I&rsquo;m going to implement Huang and Langston&rsquo;s block sort much like
Andrey did. My original goal was to create a block sort implementation suitable
for inclusion in Rust&rsquo;s standard library.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> I&rsquo;m not the first to
try this; the <a href=https://github.com/HolyGrailSortProject/Rewritten-Grailsort target=_blank>Rewritten Grailsort project</a> has several
translations of Andrey&rsquo;s implementation in various languages (including
Rust). However, this is the first one I know of that uses the block tagging
scheme described by Huang and Langston, which halves the requisite length
of the internal buffer compared to other implementations. I&rsquo;m going to
describe this method, as well as the rest of the algorithm, in detail.
Hopefully by expanding on the parts I had difficulty with, others will be
able to learn about an interesting corner of computer science.</p><h2 id=merging-in-place><a class=heading-link href=#merging-in-place>Merging In-Place</a></h2><p>A block sort is fundamentally a merge sort. It repeatedly merges two sorted
subsequences (runs) until the entire input is sorted. Unlike a typical merge
sort, however, we&rsquo;re not allowed to use an external buffer, so we&rsquo;re going to
need a merge subroutine that works in-place. Unfortunately without <em>any</em>
auxiliary storage, merging takes quadratic time. We need to find some sort of
workaround.</p><h3 id=mᴇʀɢᴇbᴜғ><a class=heading-link href=#mᴇʀɢᴇbᴜғ>MᴇʀɢᴇBᴜғ</a></h3><p>To achieve a linear-time merge, we first extract a subset of elements from
the initial input that can be arbitrarily permuted without compromising the
overall stability of the sort. Assuming we have collected a contiguous sequence
of such elements—call this the &ldquo;internal buffer&rdquo;— we can merge two contiguous
sequences $A$ and $B$ by repeatedly comparing the leftmost element from each
and swapping the lesser one with the leftmost element in the internal buffer.</p><figure><img src=/images/grailsort/merge-with-buffer.png alt="Two steps of an in-place merge using an internal buffer."><figcaption><p>Two steps of an in-place merge using an internal buffer.</p></figcaption></figure><p>Dotted lines indicate searches or comparisons, while solid lines indicate
movement—in this case swaps.</p><p>This requires only that the internal buffer is at least as long as $B$. There
are no requirements on the length of $A$. To see why, observe that this routine
fails only when the merged elements collide with $A$. When we swap an element
from $B$, the distance between the merged elements and $A$ decreases by one,
but when we swap an element from $A$, the distance stays the same.</p><p>Furthermore, if the size of the internal buffer is equal to $|B|$, then once
one of the inputs is exhausted, the internal buffer will be contiguous in
memory with any remaining elements of $B$ to its right. These elements can be
appended to the merged ones via a single rotation, moving the internal buffer
to the right of the merged sequence.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></p><h5 id=complexity><a class=heading-link href=#complexity>Complexity</a></h5><p>It should be obvious that this takes linear time. Each step merges one element
and requires exactly one comparison and one swap. Also, note that we can
construct a mirrored version of this subroutine, one where the internal buffer
starts on the right and moves to the left. In the mirrored version, $A$ and $B$
are processed from right to left (greatest to least). I&rsquo;ll refer to the two
versions as MᴇʀɢᴇBᴜғLᴇғᴛ and MᴇʀɢᴇBᴜғRɪɢʜᴛ, where the direction indicates the
side on which the internal buffer starts.</p><h3 id=mᴇʀɢᴇnᴏbᴜғ><a class=heading-link href=#mᴇʀɢᴇnᴏbᴜғ>MᴇʀɢᴇNᴏBᴜғ</a></h3><p>MᴇʀɢᴇBᴜғ is not enough on its own, however. At some point, we will need to
merge the internal buffer back into the rest of the input. This will require
an in-place merge with <em>zero</em> auxiliary storage. Let&rsquo;s call it MᴇʀɢᴇNᴏBᴜғ. As
discussed earlier, this routine cannot run in linear time, but by using it only
in limited circumstances, we can still guarantee an overall runtime of $O(n
\log{} n)$.</p><p>MᴇʀɢᴇNᴏBᴜғ executes the following steps repeatedly.</p><ol><li>Do a binary search in $B$ to find the index that the leftmost element in
$A$ would occupy.</li><li>Search forward in $A$ linearly to find all elements equal to the leftmost one.<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></li><li>Rotate all elements between the locations found in steps 1 and 2 to the
right by the index found in step one.</li></ol><p>Each step puts at least one element from $A$ in its final, merged position. It
continues until one of the sequences is exhausted.</p><figure><img src=/images/grailsort/merge-no-buffer.png alt="A single step of an in-place merge with no auxiliary buffer."><figcaption><p>A single step of an in-place merge with no auxiliary buffer.</p></figcaption></figure><p>First, we find the appropriate position in $B$ for the ⚁ at the start of $A$.
Next we find the number of duplicates of that element (2). Finally, we rotate
all elements in $B$ less than ⚁—in this case, a single ⚀—into their final
position at the start of the input. At the end of this iteration, three
elements have been merged in total.</p><h5 id=complexity-1><a class=heading-link href=#complexity-1>Complexity</a></h5><p>It should be clear that the number of iterations is bounded by the number of
distinct elements in $A$. Call these $A_{\ne}$. Step 1 requires a binary
search in $B$, and $B$ may or may not shrink at each iteration, therefore the
number of comparisons is $O(|A_{\ne}| \log{} |B|)$. Step 2 will only
visit each element in $A$ exactly once across all iterations, so its complexity
is $O(|A|)$.</p><p>Step 3 is more complex. Each iteration involves a <a href=https://doc.rust-lang.org/std/primitive.slice.html#complexity-1 target=_blank>linear-time</a>
rotation of some elements of $A$. Much like in step 2, each element of $B$ is
part of no more than one rotation, so the complexity of this step across
all iterations is $O(|A_{\ne}||A| + |B|)$.<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup></p><p>Combine these and simplify to get a total complexity of
$O(|A_{\ne}||A| + |A_{\ne}| \log{} |B| + |B|)$. Assuming that $A$ has
many distinct elements ($|A_{\ne}| \approx |A|$), this operation is quadratic in
$|A|$ but linear in $|B|$.</p><p>As with MᴇʀɢᴇBᴜғ, a mirrored version exists that is quadratic in $|B|$ instead.
I&rsquo;ll refer to the version illustrated above as MᴇʀɢᴇNᴏBᴜғLᴇғᴛ and the mirrored
version as MᴇʀɢᴇNᴏBᴜғRɪɢʜᴛ. The direction indicates which input is
quadratic. MᴇʀɢᴇNᴏBᴜғRɪɢʜᴛ can be implemented by rotating all duplicates of the
<em>rightmost</em> element of $B$ into the correct position in $A$ at each iteration.</p><h2 id=grailsort><a class=heading-link href=#grailsort>Grailsort</a></h2><p>Finally, we&rsquo;re ready to implement our in-place, stable sort.</p><p>Our algorithm proceeds like a bottom-up merge sort and divides the input (of
length $n$) into runs of length $m$, starting with $m=1$.<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> At each
step $k$ of the sort, $m=2^k$, and pairs of runs are merged into a single run
of length $2m$. Each merge must be stable, otherwise the stability of the sort
would be compromised. This continues iteratively until the entire input is
sorted ($n \le m$). As long as each step is done in $O(n)$ time—requiring that
each merge be done in $O(m)$ time—the overall runtime of the algorithm is $O(n
\log{} n)$.</p><h3 id=phase-0-setup><a class=heading-link href=#phase-0-setup>Phase 0: Setup</a></h3><p>As discussed above, MᴇʀɢᴇBᴜғ requires an internal buffer comprised of elements
that can be shuffled arbitrarily. The first step is to extract $x ≈ \sqrt{n}$
such elements. To make indexing easier, we round up to the nearest power of
two. If there are not enough suitable elements, extract as many as possible
(rounding down instead).</p><h4 id=bᴜғexᴛʀᴀᴄᴛ><a class=heading-link href=#bᴜғexᴛʀᴀᴄᴛ>BᴜғExᴛʀᴀᴄᴛ</a></h4><p>How to find such elements? This is a stable sort, after all, so we cannot
permute elements at will. However, relative order only matters for elements
that are equal. That means, if we extract a subset of elements such that</p><ul><li>No two elements in the subset are equal,</li><li>Each element in the subset appeared <em>before</em> any of its duplicates in the
original input,</li></ul><p>we can shuffle that subset arbitrarily without compromising the stability of
our sort. When merging the internal buffer back into the input, we must prefer
elements from the buffer if there are duplicates, since they appeared first
originally.</p><p>The first element in the input always fulfills the aforementioned criteria, so
we begin with an internal buffer of length 1. The buffer must remain sorted
throughout the process. BᴜғExᴛʀᴀᴄᴛ proceeds as follows until the buffer
reaches the desired size:</p><ol><li>Search forward in the remaining elements of the input until you find an
element that is not yet in the buffer. For this, do a binary search within
the internal buffer.</li><li>When a satisfactory element is found, rotate the internal buffer before that element.</li><li>Rotate that element into sorted order within the internal buffer.</li></ol><figure><img src=/images/grailsort/extract-buffer.png alt="Extracting the internal buffer"><figcaption><p>Extracting the internal buffer</p></figcaption></figure><p>In the diagram, the elements ⚀, ⚃ and ⚀ are already part of the internal
buffer, so they are ignored. However, ⚁ is not yet present, so the buffer is
rotated immediately before it, and then ⚁ is rotated into sorted order.</p><h5 id=complexity-2><a class=heading-link href=#complexity-2>Complexity</a></h5><p>Call the size of the internal buffer $x$ (for au<strong>x</strong>iliary). In the worst
case, we need to search through the entire input to find the desired number of
distinct elements, and each element causes a binary search in the internal
buffer, resulting in $O(n \log{} x)$ comparisons. As for rotations,
each element of the input that is <em>not</em> included in the internal buffer is part
of at most one. However, the elements in the internal buffer can be part of
$2x$ rotations in the worst case, making the runtime complexity due to
rotations $O(n + x^2)$.</p><p>Luckily, we only need a buffer of size $\sqrt{n}$, so the setup phase takes
$O(n \log{} n)$ time.</p><h3 id=phase-1-m--x><a class=heading-link href=#phase-1-m--x>Phase 1: $m &lt; x$</a></h3><p>With our setup complete, it&rsquo;s time to start merging runs.</p><p>In this phase, $m$ is less than or equal to the size of our auxiliary buffer,
so we can use MᴇʀɢᴇBᴜғ to merge runs directly. Since the auxiliary buffer
begins on the left side of the input, we will use repeated invocations of
MᴇʀɢᴇBᴜғLᴇғᴛ. Recall that, in addition to merging the two runs, each
invocation of MᴇʀɢᴇBᴜғLᴇғᴛ moves the auxiliary buffer to the right side of the
newly merged elements, conveniently positioning it immediately before the next
pair of runs.</p><p>Each level of the merge requires $m=2^k$ elements of the buffer, so the $N$th
level requires $\sum_{k=0}^{N} 2^k = 2^{N+1}-1$ buffer elements in total. In
the last step of this phase, when $m=\frac{x}{2}$, $x-1$ buffer elements will
have been moved to the right side of the input. If we are clever, we can move
a single buffer element to the right side of the input during Phase 0, then
execute the step using MᴇʀɢᴇBᴜғRɪɢʜᴛ instead of MᴇʀɢᴇBᴜғLᴇғᴛ. This causes the
buffer to move back to the left of the input, putting it in position for the
next phase without any extra rotations.<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup></p><h5 id=partial-blocks><a class=heading-link href=#partial-blocks>Partial Blocks</a></h5><p>Note that, if the length of the input is not itself a power of two, we may have
a run whose length is strictly less than $x$. Similarly, we may have only a
single run at the end of the input instead of a pair. It is not conceptually
difficult to handle these unusually sized runs, but it does require careful
bookkeeping. In particular, MᴇʀɢᴇBᴜғ must handle the case where the size of
the auxiliary buffer is not exactly equal to $|B|$.</p><h3 id=phase-2-x-le-m--x2><a class=heading-link href=#phase-2-x-le-m--x2>Phase 2: $x \le m &lt; x^2$</a></h3><p>At this point, runs are too large to be merged in a single step. Instead, we
split each run into a series of &ldquo;blocks&rdquo; of size $x$. We have $\frac{m}{x}$
such blocks. By carefully arranging the blocks we can use a series of MᴇʀɢᴇBᴜғ
operations to merge the runs in linear time. This phase is the crux of any
block sorting algorithm.</p><h4 id=bʟᴏᴄᴋrᴏʟʟ><a class=heading-link href=#bʟᴏᴄᴋrᴏʟʟ>BʟᴏᴄᴋRᴏʟʟ</a></h4><p>The first step is to sort all blocks in both runs by their rightmost element,
or tail. We call this &ldquo;rolling&rdquo; the blocks, a term from the 2008 paper
on which the Wikipedia article is based. This sort must be stable, and we must
also remember which blocks are from the left run and which are from the right
once they are interleaved. We&rsquo;ll call these $A$ and $B$ blocks respectively.
To preserve the stability of the merge, $A$ block elements must be preferred to
$B$ block elements if equal.</p><p>To remember which blocks are which, we treat our auxiliary buffer as a
&ldquo;movement imitation buffer&rdquo;. Before sorting blocks, we select as many elements
from the auxiliary buffer as there are blocks and sort them. Each element in
the buffer corresponds to one of the blocks, and we keep a pointer to the
element representing the first $B$ block. Every time we swap a pair of blocks
as part of our sort, we swap the corresponding pair of buffer elements, and, if
necessary, update the pointer. Since the buffer began in sorted order, all
blocks whose corresponding buffer element compares less than the pointed-to one
are $A$ blocks. The rest are $B$ blocks.</p><p>For the sorting itself, Huang and Langston use a standard selection sort. This
is sufficient to achieve our desired algorithmic complexity (see below), but a
practical implementation can and should advantage of the
fact that the $A$ blocks and $B$ blocks are in sorted order. This reduces the
number of comparisons needed to find the block with the next smallest tail. See
the source code for the fully optimized version.</p><figure><img src=/images/grailsort/sort-blocks.png alt="Use a movement imitation buffer to remember a block&amp;rsquo;s origin."><figcaption><p>Use a movement imitation buffer to remember a block&rsquo;s origin.</p></figcaption></figure><h5 id=complexity-3><a class=heading-link href=#complexity-3>Complexity</a></h5><p>Each run has at most $\frac{m}{x}$ blocks, and since the last step in this
phase merges $x^2$ elements at a time, we can have no more than $\sqrt{m}$
blocks. That means we can use a sorting algorithm with a quadratic number of
comparisons so long as it uses a linear number of swaps. Selection sort is one
such algorithm.</p><h4 id=bʟᴏᴄᴋtᴀɢ><a class=heading-link href=#bʟᴏᴄᴋtᴀɢ>BʟᴏᴄᴋTᴀɢ</a></h4><p>By sorting the blocks by their tails, we have moved them close enough to their
final position that MᴇʀɢᴇBᴜғ can finish the job. Unfortunately, MᴇʀɢᴇBᴜғ
requires the use of the auxiliary buffer, which is currently being used to remember
which blocks are $A$ and which are $B$. To free up the buffer, we&rsquo;re going to
temporarily encode this information in the blocks themselves using a novel
method from Huang and Langston.</p><p>Whenever we have two elements in a sorted sequence that are distinct, we can
encode a single bit of information by swapping them. For example, if the first
element in a block is less than the last element, we could swap the two if and
only if it is a $B$ block. Sadly, our input (and therefore our blocks) can
contain arbitrarily many duplicates, and duplicates cannot encode information in
this way.</p><p>Instead of directly encoding which blocks are which (one bit per block), we
settle for encoding where a <em>series</em> of blocks ends (one bit per series). To do
so, we take advantage of the following invariants post-BʟᴏᴄᴋRᴏʟʟ:</p><ul><li>All elements in the $A$ series remain in sorted order. Same for the $B$ series.</li><li>The tails of each block are in sorted order, with $A$ blocks to the left of
$B$ blocks when their tails are equal.</li></ul><p>As a result of the second invariant, we know that whenever an $A$ block
succeeds a $B$ block the tail of the $A$ block is <strong>strictly</strong> greater than
the tail of the $B$ block. Furthermore, we know that the tail of
<em>all</em> subsequent blocks are also strictly greater than the tail of that $B$
block, since the tails are in sorted order.</p><p>Whenever we have a series of $A$ blocks, swap the leftmost element (head) of
its immediate $B$ block predecessor with the tail of its immediate $B$ block
successor as shown below with a single $A$ block.</p><figure><img src=/images/grailsort/tag-block-tag.png alt="A swap denotes the end of one B block series and the start of the next."><figcaption><p>A swap denotes the end of one B block series and the start of the next.</p></figcaption></figure><p>You may wonder why we swap with the head of the preceding block instead of the
tail. This is to avoid collisions when the $B$ block series in question
consists of only a single block. To convince you that everything still works
correctly, I&rsquo;ve illustrated the single block case below.</p><figure><img src=/images/grailsort/tag-block-single.png alt="Swapping with the head avoids collisions in a B block series of length one."><figcaption><p>Swapping with the head avoids collisions in a B block series of length one.</p></figcaption></figure><h4 id=bʟᴏᴄᴋuɴᴛᴀɢ><a class=heading-link href=#bʟᴏᴄᴋuɴᴛᴀɢ>BʟᴏᴄᴋUɴᴛᴀɢ</a></h4><p>To better understand the BʟᴏᴄᴋTᴀɢ routine, let&rsquo;s see how
to extract the information it encoded. We&rsquo;ll do this incrementally by
scanning linearly across the blocks. Assuming we start in a $B$ block series:</p><ol><li>Compare the head of the $B$ block with its immediate successor in that
block. If the head is greater than its successor, this is the last $B$
block in the series. The next block is an $A$ block.</li><li>Compare the tail of each block with the tail of its preceding block. If the
tail of the current block is less than that of its predecessor, the current
block is the first $B$ block in a $B$ block series.</li><li>Swap the head of the block from step 1 with the tail of the block from step 2.
This undoes the swap from BʟᴏᴄᴋTᴀɢ and restores the original order.</li></ol><figure><img src=/images/grailsort/tag-block-untag.png alt="Untagging reconstructs the data from the movement imitation buffer and puts the input back in original order."><figcaption><p>Untagging reconstructs the data from the movement imitation buffer and puts the input back in original order.</p></figcaption></figure><p>Note that this scheme is unable to encode how many $A$ blocks are at the start
and end of the run: there&rsquo;s no preceding (succeeding) $B$ block. We
must pass this information out-of-band. Thankfully it is of constant size.</p><h4 id=bʟᴏᴄᴋmᴇʀɢᴇbᴜғ><a class=heading-link href=#bʟᴏᴄᴋmᴇʀɢᴇbᴜғ>BʟᴏᴄᴋMᴇʀɢᴇBᴜғ</a></h4><p>Once we have tagged the block sequence, we can repurpose the auxiliary buffer
as an internal buffer for MᴇʀɢᴇBᴜғ. Recall that the left input to MᴇʀɢᴇBᴜғLᴇғᴛ
can be arbitrarily large so long as the right input is no larger than the
internal buffer (a single block).</p><p>We proceed by merging each $A$ (or $B$) block series with the first block of
the $B$ (or $A$) block series that succeeds it. Because we sorted blocks by
their tails, we know that the final position of the tail of the right block
is to the right of any element in the left series. Therefore, MᴇʀɢᴇBᴜғLᴇғᴛ will put
all elements from the series in their final position, with at least one element
from the successor block remaining.</p><p>Unlike in Phase 1, where we were merging entire runs with a single MᴇʀɢᴇBᴜғ
invocation, we do not rotate the auxiliary buffer <a href=#fnref:4>past the remaining
elements</a> of the successor block at this point. The remaining
elements are not in their final merged position, since they may be greater than
some elements from the first block in the next block series.</p><figure><img src=/images/grailsort/merge-blocks.png alt="Merge rolled blocks by repeatedly invoking MᴇʀɢᴇBᴜғ."><figcaption><p>Merge rolled blocks by repeatedly invoking MᴇʀɢᴇBᴜғ.</p></figcaption></figure><p>The leftover elements from the first block of the right series become the first
elements of the left series for the next invocation of MᴇʀɢᴇBᴜғLᴇғᴛ. After we
merge an $AB$ series pair, we use another invocation of BʟᴏᴄᴋUɴᴛᴀɢ to figure out where to
go next. Like before, the internal buffer moves rightwards through the run until it
reaches the end.</p><h5 id=partial-blocks-1><a class=heading-link href=#partial-blocks-1>Partial Blocks</a></h5><p>As in Phase 1, we may have only a single run at the end of the input. In this
case there is no merging to be done, and we can continue to the next value of
$m$. We may also have a pair of runs where the length of the one on the right
is not exactly $m$. In this case, the final $B$ block may have size less
than $x$. We&rsquo;ll call it a partial block.</p><p>Trying to sort and tag the partial block would make it difficult to index any
$A$ blocks that are ordered after it. Instead, we ignore it until all other
blocks are merged. It is not subject to sorting or tagging. If the last series
of blocks was an $A$ series, we do one final merge with the partial $B$ block.
If the last series was a $B$ series, the partial block is already in the
correct position.</p><h3 id=phase-3-x2-le-m><a class=heading-link href=#phase-3-x2-le-m>Phase 3: $x^2 \le m$</a></h3><p>If $\sqrt{n} \le x$, then $n \le m$ at this point and we are done. However, if
we failed to find sufficiently many distinct elements in Phase 0, we have more
work to do. Specifically, we know that the number of distinct elements in the
input $u$ is bounded by $2\sqrt{n}$ and therefore $x ≈ u$.<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> By
combining the second fact with the inequality that defines this phase, we have
an even tighter bound, $u \le \sqrt{m}$.</p><p>We cannot continue as we did in Phase 2 because there are more than $\sqrt{m}$
blocks in each run and BʟᴏᴄᴋRᴏʟʟ is no longer guaranteed to run in $m$ time.
To get around this, each step will use the same number of blocks ($x$) for each
pair of runs, allowing the length of the blocks to grow arbitrarily ($m/x$). We
can continue to BʟᴏᴄᴋRᴏʟʟ as we did in phase 2, but we can no longer use
BʟᴏᴄᴋMᴇʀɢᴇBᴜғ, since the blocks will be too large to fit in the auxiliary
buffer.</p><h4 id=bʟᴏᴄᴋmᴇʀɢᴇnᴏbᴜғ><a class=heading-link href=#bʟᴏᴄᴋmᴇʀɢᴇnᴏbᴜғ>BʟᴏᴄᴋMᴇʀɢᴇNᴏBᴜғ</a></h4><p>Surprisingly, the bound on $u$ allows us to simply replace MᴇʀɢᴇBᴜғ in
BʟᴏᴄᴋMᴇʀɢᴇBᴜғ with MᴇʀɢᴇNᴏBᴜғ while still running in linear time!</p><p>Because we no longer need an internal buffer for MᴇʀɢᴇBᴜғ, we can use the
auxiliary buffer exclusively as a movement imitation buffer for the entire
phase. There is no need to call BʟᴏᴄᴋTᴀɢ or BʟᴏᴄᴋUɴᴛᴀɢ. While merging, we will
use the movement imitation buffer directly to determine which blocks are $A$
blocks and which are $B$. Everything else is exactly the same as in
BʟᴏᴄᴋMᴇʀɢᴇBᴜғ.</p><figure><img src=/images/grailsort/merge-blocks-no-buf.png alt="Merge rolled blocks by repeatedly invoking MᴇʀɢᴇNᴏBᴜғ."><figcaption><p>Merge rolled blocks by repeatedly invoking MᴇʀɢᴇNᴏBᴜғ.</p></figcaption></figure><h5 id=complexity-4><a class=heading-link href=#complexity-4>Complexity</a></h5><p>To quote Huang and Langston, &ldquo;although this method is easy to implement, it is
not perhaps obvious that it takes only linear time&rdquo;. Recall that the
complexity for MᴇʀɢᴇNᴏBᴜғRɪɢʜᴛ was $O(|R_{\ne}||R| + |R_{\ne}| \log{} |L| +
|L|)$, where $L$ and $R$ are the sequences on the left and right
respectively.<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup></p><p>Each BʟᴏᴄᴋMᴇʀɢᴇNᴏBᴜғ operates on a single pair of runs and may involve multiple
invocations of MᴇʀɢᴇNᴏBᴜғ. A single element will be part of no more than two
MᴇʀɢᴇNᴏBᴜғ invocations (once on the left, once on the right). Recall that the
factor $|R_{\ne}|$—the number of distinct elements in the right sequence—is
what determines the number of iterations through the inner loop of MᴇʀɢᴇNᴏBᴜғ.
We&rsquo;re going to add up all these iterations to compute the complexity of BʟᴏᴄᴋMᴇʀɢᴇNᴏBᴜғ.</p><p>The left input to MᴇʀɢᴇNᴏBᴜғ may be arbitrarily long, so $|L|$ becomes $m$ in the
worst-case, while the right input is always exactly one block long and
becomes $\frac{m}{x}$. $|R_{\ne}|$ is bounded by $u$, the number of distinct
elements in the input. However, distinct elements can be split across several
block series, meaning the same value can appear in the right sequence of a
MᴇʀɢᴇNᴏBᴜғ operation multiple times. Thankfully, each distinct value may appear
in <strong>no more than four MᴇʀɢᴇNᴏBᴜғ operations</strong>, so we can substitute $u$ for
$|R_{\ne}|$ when computing the total runtime across all iterations.</p><p>Why four? Since runs are sorted, all equal elements appear consecutively within
their run. Therefore, a single value can be present in two locations:</p><ul><li>inside any number of consecutive blocks whose tail is equal to that value.</li><li>inside zero or one blocks whose tail is not equal to that value.</li></ul><p>BʟᴏᴄᴋRᴏʟʟ, because it is stable, keeps blocks from one run whose tails are
equal together in a single series, so all blocks from the first case remain
contiguous. This gives us a maximum of two block series in each run that may contain a
given element, for a total of four.</p><p>I&rsquo;ve illustrated the worst-case scenario below for an input with many ⚂s.
You should be able to convince yourself that BʟᴏᴄᴋRᴏʟʟ cannot result in a fifth
block series containing a ⚂.</p><figure><img src=/images/grailsort/block-distinct.png alt="An input with many ⚂s, which can be split across no more than four block series."><figcaption><p>An input with many ⚂s, which can be split across no more than four block series.</p></figcaption></figure><p>It&rsquo;s time to apply these rules to prove that BʟᴏᴄᴋMᴇʀɢᴇNᴏBᴜғ runs in $O(m)$
time. Sum up all MᴇʀɢᴇNᴏBᴜғ invocations using the rules above, then use the
fact that $u ≈ x \le \sqrt{m}$ and $\log{} m ∈ O(\sqrt{m})$.</p><p>\begin{align*}
\sum O(|R_{\ne}||R| + |R_{\ne}| \log{} |L| + |L|)
& = O(u\frac{m}{x} + u \log{} m + m) \cr
& \approx O(x\frac{m}{x} + x \log{} m + m) \cr
& \le O(x\frac{m}{x} + \sqrt{m} (\sqrt{m}) + m) \cr
& = O(m)
\end{align*}</p><p>Voila!</p><h3 id=phase-4-cleanup><a class=heading-link href=#phase-4-cleanup>Phase 4: Cleanup</a></h3><p>Merging continues until all of the input is part of a single, sorted run except
for the part in the internal buffer. Because the buffer has length
$O(\sqrt{n})$, we can sort the internal buffer using a quadratic algorithm,
then call MᴇʀɢᴇNᴏBᴜғ to merge it into the input. Remember that elements in
the internal buffer came before any of their duplicates in the original
input, so they should be preferred when merging to preserve stability.</p><p>The input is now sorted!</p><h2 id=conclusion><a class=heading-link href=#conclusion>Conclusion</a></h2><p>With that, we&rsquo;ve finished our own holy sorting grail. Honestly I find it
miraculous that this is even possible. But it&rsquo;s a lot of work just to sort a
list!</p><p>I&rsquo;ve called my implementation
<a href=https://github.com/ecstatic-morse/MrrlSort target=_blank>MrrlSort</a> to pay tribute to Andrey
(and because it sounds a bit like &ldquo;grail&rdquo;). It&rsquo;s tested using
<a href=https://docs.rs/proptest/latest/proptest/ target=_blank>proptest</a>, a property-based testing
framework that found several subtle bugs during development. Now that it passes
those tests, I&rsquo;m pretty confident in its correctness.</p><p>In a simple benchmark, MrrlSort took about twice as long as Rust&rsquo;s <code>sort</code>. Not
bad! Given the number of swaps, rotations, and auxiliary buffer operations I
expected worse. I&rsquo;m very curious to see how this factor is affected by various
parameters (size of elements, cost of comparisons, etc.). I suspect it can be
reduced further, but I think I need to take a break for now. To
paraphrase Gankra, whose offhand mention of Grailsort in <a href=https://github.com/rust-lang/rust/issues/19221#issuecomment-70445816 target=_blank>a comment from 2015</a>
sent me on this unexpected diversion (and who is also known to publish the
occasional <a href=https://gankra.github.io/blah/fix-rust-pointers/ target=_blank>massive blog post</a>):</p><p>Head empty, only block sort.</p><style>.post-content #dedication{display:none}.post-content #dedication+p{background-color:#add8e6;border:dotted #000 2px;padding:8px;margin-bottom:2em}body[data-theme=dark] .post-content #dedication+p{background-color:#000;border:dotted grey 2px;padding:8px;margin-bottom:2em}@media(prefers-color-scheme:dark){body[data-theme=auto] .post-content #dedication+p{background-color:#000;border:dotted grey 2px;padding:8px;margin-bottom:2em}}.post-content .heading-link{color:var(--color-primary)}figure>img{display:block;margin-left:auto!important;margin-right:auto!important}figcaption>p{text-align:center;color:var(--color-mute)}.katex-html{white-space:nowrap}</style><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>In a paper called <a href=http://i.stanford.edu/pub/cstr/reports/cs/tr/74/470/CS-TR-74-470.pdf target=_blank>&ldquo;Stable Sorting and Merging with Optimal Space and Time Bounds&rdquo;</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>See <a href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.8381" target=_blank>&ldquo;Fast Stable Merging and Sorting in Constant Extra Space&rdquo;</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>I&rsquo;m not convinced that block sort is an appropriate choice
for general purpose programming languages, but it&rsquo;s nice to have the
option!</p><p>I first became interested in the subject when I learned of the
effort to include Rust inside the Linux kernel. One obstacle to running
Rust in kernel-space is that the standard library assumes that allocation
failure is unrecoverable and results in process termination. This
assumption is invalid inside a kernel, which makes developing them in Rust
unpleasant. Many functions in the standard library must be
rewritten to propagate allocator errors to the caller.</p><p>Amusingly, one such function is <a href=https://doc.rust-lang.org/std/primitive.slice.html#method.sort target=_blank><code>sort</code></a>! <code>sort</code> is guaranteed
to be stable in Rust and implemented via an optimized merge sort, which
needs to allocate. I suspect most users expect <code>sort</code> to succeed
unconditionally, and would not know what to do if it returned an error.
<a href=https://ziglang.org/ target=_blank>Zig</a>, which forces programmers to consider the possibility of allocation
failure and also guarantees stability for <a href=https://ziglang.org/documentation/master/std/#std;sort.sort target=_blank><code>sort</code></a>, uses block
sort for its implementation.</p><p>Empirically, most users do not need a stable sorting algorithm, so standard
library <code>sort</code> should probably default to a fast, constant-space, unstable
one, avoiding the overhead of block sort. If needed, a stable algorithm can
be requested specifically, a lá C++&rsquo;s <a href=https://en.cppreference.com/w/cpp/algorithm/stable_sort target=_blank><code>stable_sort</code></a>.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>This detail will be important later.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>This version of MᴇʀɢᴇNᴏBᴜғ is not fully optimized. For example, Step 2
could use an <a href=https://en.wikipedia.org/wiki/Exponential_search target=_blank>exponential search</a> instead of a linear one, or
it could find the rightmost element in $B$ whose final position is the same
as the one found in step 1 instead of stopping at duplicates. However,
these optimizations are not necessary to achieve the desired runtime and
they complicate analysis, so I won&rsquo;t discuss them further.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>If you find this too hand-wavy, here&rsquo;s a more formal
description. The $k$-th rotation has complexity $O(|A_k| +
|B_k|)$, where $A_k$ are the elements in that rotation originating from
$A$, etc. Add this up over all rotations to get $\sum_{k=0}^{|A_{\ne}|}
O(|A_k| + |B_k|)$ and sum the two terms separately.</p><p>For the first sum, replace $|A_k|$ by a conservative upper bound $|A|$ to
obtain $O(|A_{\ne}||A|)$.</p><p>For the second, as we discussed above, no element in $B$ is rotated more
than once, so all the $B_k$s are mutually disjoint and the sum can be
simplified to $O(|B|)$.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7 role=doc-endnote><p>Instead of starting with $m=1$, a practical merge (or block) sort
will use a quadratic algorithm (usually insertion sort), to sort runs up to
some small, fixed size.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8 role=doc-endnote><p>Alternatively, you can rotate the buffer elements back to the
start of the input, complete the level with MᴇʀɢᴇBᴜғLᴇғᴛ as before, then
rotate them back a second time to prepare for the next phase. This is what
the current implementation does.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9 role=doc-endnote><p>That&rsquo;s because <a href=#phase-0-setup>Phase 0</a> picks as many distinct
elements as it can up to $\sqrt{n}$.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10 role=doc-endnote><p>This was originally expressed in terms of $A$ and $B$, but in
this phase we&rsquo;re using those letters to denote the origin of a block
series. In this phase, every alternate MᴇʀɢᴇNᴏBᴜғ invocation has a $B$
block series on the left and a single $A$ block on the right.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></article></div><aside class="col-12 col-md-3 float-left sidebar"><div class="sidebar-item sidebar-pages"><h3>Pages</h3><ul></ul></div><div class="sidebar-item sidebar-links"><h3>Links</h3><ul></ul></div><div class="sidebar-item sidebar-tags"><h3>Tags</h3><div></div></div><div class="sidebar-item sidebar-toc"><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><ul><li><ul><li></li></ul></li></ul></li><li><a href=#introduction>Introduction</a></li><li><a href=#merging-in-place>Merging In-Place</a><ul><li><a href=#mᴇʀɢᴇbᴜғ>MᴇʀɢᴇBᴜғ</a><ul><li></li></ul></li><li><a href=#mᴇʀɢᴇnᴏbᴜғ>MᴇʀɢᴇNᴏBᴜғ</a><ul><li></li></ul></li></ul></li><li><a href=#grailsort>Grailsort</a><ul><li><a href=#phase-0-setup>Phase 0: Setup</a><ul><li><a href=#bᴜғexᴛʀᴀᴄᴛ>BᴜғExᴛʀᴀᴄᴛ</a></li></ul></li><li><a href=#phase-1-m--x>Phase 1: $m &lt; x$</a><ul><li></li></ul></li><li><a href=#phase-2-x-le-m--x2>Phase 2: $x \le m &lt; x^2$</a><ul><li><a href=#bʟᴏᴄᴋrᴏʟʟ>BʟᴏᴄᴋRᴏʟʟ</a></li><li><a href=#bʟᴏᴄᴋtᴀɢ>BʟᴏᴄᴋTᴀɢ</a></li><li><a href=#bʟᴏᴄᴋuɴᴛᴀɢ>BʟᴏᴄᴋUɴᴛᴀɢ</a></li><li><a href=#bʟᴏᴄᴋmᴇʀɢᴇbᴜғ>BʟᴏᴄᴋMᴇʀɢᴇBᴜғ</a></li></ul></li><li><a href=#phase-3-x2-le-m>Phase 3: $x^2 \le m$</a><ul><li><a href=#bʟᴏᴄᴋmᴇʀɢᴇnᴏbᴜғ>BʟᴏᴄᴋMᴇʀɢᴇNᴏBᴜғ</a></li></ul></li><li><a href=#phase-4-cleanup>Phase 4: Cleanup</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></aside></div><div class=btn><div class=btn-menu id=btn-menu><i class="iconfont icon-grid-sharp"></i></div><div class=btn-toggle-mode><i class="iconfont icon-contrast-sharp"></i></div><div class=btn-scroll-top><i class="iconfont icon-chevron-up-circle-sharp"></i></div></div><aside class=sidebar-mobile style=display:none><div class=sidebar-wrapper><div class="sidebar-item sidebar-pages"><h3>Pages</h3><ul></ul></div><div class="sidebar-item sidebar-links"><h3>Links</h3><ul></ul></div><div class="sidebar-item sidebar-tags"><h3>Tags</h3><div></div></div></div></aside></main><footer><div class="container-lg clearfix"><div class="col-12 footer"><p>Unless otherwise noted, the content of this site is licensed under <a rel=license href=https://creativecommons.org/licenses/by/4.0/legalcode target=_blank>CC BY 4.0</a>.</p><span>&copy; 2022
<a href=//ecstaticmorse.net/>Dylan MacKenzie</a>
| Powered by <a href=https://github.com/dsrkafuu/hugo-theme-fuji/ target=_blank>Fuji-v2</a> & <a href=https://gohugo.io/ target=_blank>Hugo</a></span></div></div></footer><script defer src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/prismjs@1.27.0/components/prism-core.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/prismjs@1.27.0/plugins/autoloader/prism-autoloader.min.js></script>
<script defer src=/assets/js/fuji.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css><script src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js></script>
<script>var macros={"\\sub":"\\subseteq","\\sup":"\\supseteq"},delimiters=[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{align*}",right:"\\end{align*}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{gather*}",right:"\\end{gather*}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}];document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters,macros,fleqn:!0})})</script></body></html>